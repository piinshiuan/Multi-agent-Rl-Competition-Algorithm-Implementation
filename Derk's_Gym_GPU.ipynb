{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p3VFxSKfgx96",
        "outputId": "4fb3d9f4-889b-450a-84c9-06d2914d13c1"
      },
      "outputs": [],
      "source": [
        "!wget -O \"virtualgl_2.6.4_amd64.deb\" \"https://sourceforge.net/projects/virtualgl/files/2.6.4/virtualgl_2.6.4_amd64.deb/download\"\n",
        "!dpkg -i virtualgl_2.6.4_amd64.deb\n",
        "!printf '#!/bin/bash\\nVGL_DEFAULTFBCONFIG=GLX_ALPHA_SIZE,8 DISPLAY=:0 vglrun +v /usr/bin/chromium-browser --disable-gpu-sandbox \"$@\"' > chromium-vgl.sh && chmod a+x ./chromium-vgl.sh\n",
        "!pip install git+https://github.com/demotomohiro/remocolab.git\n",
        "import remocolab\n",
        "remocolab._setup_nvidia_gl()\n",
        "\n",
        "!pip install gym-derk\n",
        "\n",
        "# This is a hack to get pyppeteer working, suggested in: https://github.com/googlecolab/colabtools/issues/1553\n",
        "from google.colab import output\n",
        "with output.temporary():\n",
        "  !pip install pyppeteer tornado==4.5.3 \n",
        "  !apt update\n",
        "  !apt install chromium-chromedriver\n",
        "\n",
        "import os\n",
        "import signal\n",
        "\n",
        "# Kill the process to restart with the older tornado version.\n",
        "os.kill(os.getpid(), signal.SIGTERM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km63Sny_-I5W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['DERK_CHROME_EXECUTABLE'] = './chromium-vgl.sh'\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from gym_derk.envs import DerkEnv\n",
        "from gym_derk import ObservationKeys\n",
        "from gym_derk import ActionKeys\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.optim as optim\n",
        "import asyncio\n",
        "import json\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNZRyhsJefGD"
      },
      "outputs": [],
      "source": [
        "class Memory:\n",
        "    def __init__(self, max_length=30000):\n",
        "        self.max_length = max_length\n",
        "        self.state_buffer = np.zeros((self.max_length, 64*3)) # 64 = state_space * 3 players per team\n",
        "        self.action_buffer = np.zeros((self.max_length, 5*3)) # 13 = action_space_list * 3 players per team\n",
        "        self.reward_buffer = np.zeros((self.max_length))\n",
        "        self.nextstate_buffer = np.zeros((self.max_length, 64*3))\n",
        "        self.counter = 0\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.counter\n",
        "    \n",
        "    def push(self, obs, action, reward, obs_):\n",
        "        self.counter = (self.counter+1) % self.max_length\n",
        "        self.state_buffer[self.counter] = obs\n",
        "        self.action_buffer[self.counter] = action\n",
        "        self.reward_buffer[self.counter] = reward\n",
        "        self.nextstate_buffer[self.counter] = obs_\n",
        "        \n",
        "    def lookup_history(self, V, obs, action, buffer, gamma=0.9):\n",
        "        idx_obs = np.where((self.state_buffer==obs).all(1))[0]\n",
        "        idx_action = np.where((self.action_buffer==action).all(1))[0]\n",
        "        \n",
        "        idx = np.intersect1d(idx_obs, idx_action)\n",
        "        if idx.shape[0]==0:\n",
        "            return torch.tensor([0.], requires_grad=True)\n",
        "        \n",
        "        \n",
        "        s = buffer.state_buffer[idx]\n",
        "        state = np.concatenate([self.state_buffer[idx], s], axis=1)\n",
        "        \n",
        "        payoff = torch.FloatTensor(self.reward_buffer[idx])\n",
        "        v_out = V(torch.FloatTensor(state))\n",
        "        \n",
        "        return (torch.sum(payoff)+gamma*torch.sum(v_out)) / idx.shape[0]\n",
        "        \n",
        "    def clear(self):\n",
        "        self.counter = 0\n",
        "        self.state_buffer = np.zeros((self.max_length, 64*3))\n",
        "        self.action_buffer = np.zeros((self.max_length, 5*3))\n",
        "        self.reward_buffer = np.zeros((self.max_length))\n",
        "        self.nextstate_buffer = np.zeros((self.max_length, 64*3))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3xDTpivaRuL"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module): \n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.policynetwork = nn.Sequential(\n",
        "            nn.Linear(self.state_size*3, 128),\n",
        "            nn.Linear(128, 256), \n",
        "            nn.Linear(256, 13*3))\n",
        "\n",
        "    def forward(self, state):\n",
        "        outputs = self.policynetwork(torch.FloatTensor(state))\n",
        "        outputs = outputs.detach().numpy()\n",
        "        action=[]\n",
        "        for i in range(3):\n",
        "            start=13*i\n",
        "            casts = outputs[start+3:start+6]\n",
        "            cast_i = np.argmax(casts)\n",
        "            focuses = outputs[start+6:start+13]\n",
        "            focus_i = np.argmax(focuses)\n",
        "            move_x = max(min(math.tanh(outputs[0+start])+ np.random.uniform(-0.1,0.1),1),-1)\n",
        "            rotate = max(min(math.tanh(outputs[0+start])+ np.random.uniform(-0.1,0.1),1),-1)\n",
        "            action.append((\n",
        "              move_x, # MoveX\n",
        "              rotate, # Rotate\n",
        "              max(min(outputs[2+start] + np.random.uniform(-0.1,0.1), 1), 0), # ChaseFocus\n",
        "              (cast_i + 1) if casts[cast_i] > 0 else 0, # CastSlot\n",
        "              (focus_i + 1) if focuses[focus_i] > 0 else 0, # Focus\n",
        "            ))\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY2jCT28eh49"
      },
      "outputs": [],
      "source": [
        "def calculate_rho(state_a, state_b, V, a, b, gamma=0.9):\n",
        "    # only check one side of the state\n",
        "    idx_obsA = np.where((a.state_buffer==state_a).all(1))[0]\n",
        "    idx_obsB = np.where((b.state_buffer==state_b).all(1))[0]\n",
        "    \n",
        "    idx = np.intersect1d(idx_obsA, idx_obsB)\n",
        "    \n",
        "    if idx.shape[0]==0:\n",
        "        return torch.tensor([0.], requires_grad=True)\n",
        "\n",
        "    \n",
        "    reward_a = torch.FloatTensor(a.reward_buffer[idx_obsA])\n",
        "    nextstate_a = a.nextstate_buffer[idx]\n",
        "    nextstate_b = b.nextstate_buffer[idx]\n",
        "    \n",
        "    nextstate = np.concatenate([nextstate_a, nextstate_b], axis=1)\n",
        "\n",
        "    value = V(torch.FloatTensor(nextstate))\n",
        "    \n",
        "    return (torch.sum(reward_a)+gamma*torch.sum(value))/idx_obsA.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-BYIO56e0Mv"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.policynetwork = nn.Sequential(\n",
        "            nn.Linear(self.state_size*6, 256),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Linear(64, 1))\n",
        "\n",
        "    def forward(self, state):\n",
        "        output = self.policynetwork(state)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p68zRK9kBQZR"
      },
      "outputs": [],
      "source": [
        "env = DerkEnv(\n",
        "    home_team = [{ 'primaryColor': '#009999', 'slots':['Pistol', None, None]}],\n",
        "    away_team = [{ 'primaryColor': '#ff3333', 'slots':['Pistol', None, None]}],\n",
        "    reward_function={\n",
        "        'damageEnemyStatue': 1,\n",
        "        'damageEnemyUnit': 1,\n",
        "        'killEnemyStatue': 10,\n",
        "        'killEnemyUnit': 5,\n",
        "        'friendlyFire': -1,\n",
        "        'teamSpirit': 2,\n",
        "        'fallDamageTaken': -2\n",
        "    },\n",
        "    turbo_mode = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwMAh-U1euzp"
      },
      "outputs": [],
      "source": [
        "trajectories = 100\n",
        "decimal_places = 2\n",
        "gamma = 0.99\n",
        "epsilon = 0.2\n",
        "eta = 1e-4\n",
        "buffer_size = 2000\n",
        "logs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFwMeojHOai5",
        "outputId": "f0330a0a-5ae2-4546-f4c2-d122b9bc734a"
      },
      "outputs": [],
      "source": [
        "asyncio.get_event_loop().run_until_complete(env.app.page.screenshot(path='test.png'))\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "# os.chdir('/content/drive/MyDrive/1class 109下課程/人工智慧概論/hw_4') \n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks') \n",
        "os.listdir() #確認目錄內容\n",
        "\n",
        "actor_a = torch.load(\"./model/actor_A_4_0.99.pth\")#ctor(len(ObservationKeys),len(ActionKeys))\n",
        "actor_b = torch.load(\"./model/actor_B_4_0.99.pth\")#Actor(len(ObservationKeys),len(ActionKeys))\n",
        "v = torch.load(\"./model/ValueNet_4_0.99.pth\")#Critic(len(ObservationKeys),len(ActionKeys))\n",
        "\n",
        "\n",
        "# initialize memory buffer for lookup\n",
        "buffer_a = Memory(max_length=buffer_size)\n",
        "buffer_b = Memory(max_length=buffer_size)\n",
        "\n",
        "# list to record total reward\n",
        "# with open('a_4_0.99.txt', newline='') as jsonfile:\n",
        "#     memA = json.load(jsonfile)\n",
        "# with open('b_4_0.99.txt', newline='') as jsonfile:\n",
        "#     memB = json.load(jsonfile)\n",
        "\n",
        "print(\"already trained {} times\".format(len(memA)))\n",
        "memA = []\n",
        "memB = []\n",
        "\n",
        "# initialize loss function for V-network\n",
        "MSE = nn.MSELoss()\n",
        "\n",
        "# initialize optimizers\n",
        "optimizer_actor_a = optim.Adam(actor_a.parameters())\n",
        "optimizer_actor_b = optim.Adam(actor_b.parameters())\n",
        "optimizer_v = optim.Adam(v.parameters())\n",
        "\n",
        "for t in range(trajectories):\n",
        "    observation_n = env.reset()\n",
        "    total_reward_A = 0\n",
        "    total_reward_B = 0\n",
        "    print(\"trajectory: {}\".format(t))\n",
        "    step = 1\n",
        "    reward = []\n",
        "\n",
        "    #buffer for this round\n",
        "    temp_buffer_a = Memory()\n",
        "    temp_buffer_b = Memory()\n",
        "\n",
        "    if len(memA)>=3000:\n",
        "      print(\"finish training\")\n",
        "      break\n",
        "    while True:\n",
        "        \n",
        "        state = observation_n.reshape(64*6)\n",
        "        \n",
        "        state_a = observation_n[0:3].reshape(64*3)\n",
        "        state_b = observation_n[3:6].reshape(64*3)\n",
        "\n",
        "        state = np.around(state, decimal_places)\n",
        "        state_a = np.around(state_a, decimal_places)\n",
        "        state_b = np.around(state_b, decimal_places)\n",
        "        \n",
        "        # dim = 13, save this as action\n",
        "        if np.random.uniform()>epsilon:        \n",
        "            action_a = actor_a(state_a)\n",
        "            action_b = actor_b(state_b)\n",
        "        else:\n",
        "            one_two = np.random.uniform(-1,1,(3, 2))\n",
        "            three = np.random.uniform(size=(3, 1))\n",
        "            four = np.random.randint(4, size=(3, 1))\n",
        "            five = np.random.randint(8, size =(3, 1))\n",
        "            action_a = np.concatenate([one_two, three, four, five], axis=1)\n",
        "            \n",
        "            one_two = np.random.uniform(-1,1,(3, 2))\n",
        "            three = np.random.uniform(size=(3, 1))\n",
        "            four = np.random.randint(4, size=(3, 1))\n",
        "            five = np.random.randint(8, size =(3, 1))\n",
        "            action_b = np.concatenate([one_two, three, four, five], axis=1)\n",
        "            \n",
        "        action_a = np.around(action_a, decimal_places)\n",
        "        action_b = np.around(action_b, decimal_places)   \n",
        "        _action_a = np.asarray(action_a).reshape(15,)\n",
        "        _action_b = np.asarray(action_b).reshape(15,)\n",
        "\n",
        "        action_n = [action_a[0], action_a[1], action_a[2], action_b[0], action_b[1], action_b[2]]\n",
        "        new_observation_n, reward_n, done_n, info = env.step(action_n)\n",
        "        new_obs_a, new_obs_b = new_observation_n[:3].reshape(64*3), new_observation_n[3:].reshape(64*3)\n",
        "\n",
        "        reward_a = reward_n[0] + reward_n[1] + reward_n[2]\n",
        "        reward_b = reward_n[3] + reward_n[4] + reward_n[5]\n",
        "        \n",
        "        \n",
        "        # to zero-sum game\n",
        "        ra = reward_a\n",
        "        rb = reward_b\n",
        "        reward_a = ra - rb\n",
        "        reward_b = rb - ra\n",
        "        reward.append(reward_a)\n",
        "        \n",
        "        total_reward_A += reward_a\n",
        "        total_reward_B += reward_b\n",
        "        \n",
        "        buffer_a.push(state_a, _action_a, reward_a, new_obs_a)\n",
        "        buffer_b.push(state_b, _action_b, reward_b, new_obs_b)\n",
        "        temp_buffer_a.push(state_a, _action_a, reward_a, new_obs_a)\n",
        "        temp_buffer_b.push(state_b, _action_b, reward_b, new_obs_b)\n",
        "        \n",
        "\n",
        "        observation_n=new_observation_n\n",
        "\n",
        "        if t%logs==0:\n",
        "            torch.save(actor_a, \"./model/actor_A_4_0.99.pth\")\n",
        "            torch.save(actor_b, \"./model/actor_B_4_0.99.pth\")\n",
        "            torch.save(v, \"./model/ValueNet_4_0.99.pth\")\n",
        "            \n",
        "        if all(done_n):\n",
        "\n",
        "            print(total_reward_A)\n",
        "            memA.append(total_reward_A)\n",
        "            \n",
        "            \n",
        "            memB.append(total_reward_B)\n",
        "            print(\"total reward A:{}, total reward B:{}\".format(total_reward_A, total_reward_B))\n",
        "            print(\"Episode finished\\n\")\n",
        "            with open('a_4_0.99.txt', 'w+') as outfile:\n",
        "              json.dump(memA, outfile)\n",
        "            with open('b_4_0.99.txt', 'w+') as outfile2:\n",
        "              json.dump(memB, outfile2)\n",
        "            #update\n",
        "            acumulated_reward = 0\n",
        "            for i in range(len(reward)-1,-1,-1):\n",
        "                acumulated_reward = acumulated_reward*gamma + reward[i]\n",
        "                state_a = temp_buffer_a.state_buffer[i]\n",
        "                state_b = temp_buffer_b.state_buffer[i]           \n",
        "                \n",
        "                _action_a = temp_buffer_a.action_buffer[i]\n",
        "                _action_b = temp_buffer_b.action_buffer[i]\n",
        "                la = buffer_a.lookup_history(v, state_a, _action_a, buffer_b, gamma)\n",
        "                rb = buffer_b.lookup_history(v, state_b, _action_b, buffer_a, gamma)\n",
        "                state_dict_a = actor_a.state_dict()\n",
        "                state_dict_b = actor_b.state_dict()\n",
        "    \n",
        "                for name, param in state_dict_a.items():\n",
        "                    transformed_param = param + la * eta\n",
        "                    state_dict_a[name].copy_(transformed_param)\n",
        "                \n",
        "                \n",
        "                for name, param in state_dict_b.items():\n",
        "                    transformed_param = param + rb * eta\n",
        "                    state_dict_b[name].copy_(transformed_param)\n",
        "\n",
        "                critic_loss = MSE(\n",
        "                    v(torch.FloatTensor(state)), \n",
        "                    torch.tensor(acumulated_reward, dtype=torch.float, requires_grad=True)\n",
        "                )\n",
        "                optimizer_v.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                optimizer_v.step()\n",
        "\n",
        "            \n",
        "            break\n",
        "    buffer_a.clear()\n",
        "    buffer_b.clear()\n",
        "\n",
        "    \n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "IfEDA-Ij_gLz",
        "outputId": "9a896f51-92eb-4510-ba3b-adceb32964ba"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import json\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks') \n",
        "os.listdir() #確認目錄內容\n",
        "with open('a_v3.txt', newline='') as jsonfile:\n",
        "    memA = json.load(jsonfile)\n",
        "with open('b_v3.txt', newline='') as jsonfile:\n",
        "    memB = json.load(jsonfile)\n",
        "plt.plot(memA,'b')\n",
        "plt.title(\"Total reward blue A\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(memB,'r')\n",
        "plt.title(\"Total reward red B \")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Derk's Gym GPU2_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
